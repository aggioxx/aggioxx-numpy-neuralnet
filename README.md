This code implements a simple neural network from scratch to recognize handwritten digits from the MNIST dataset. Here's a breakdown of the main components:  
Data Preparation
Loads MNIST digit data from CSV
Splits into training (most of the data) and testing sets (1000 samples)
Data is shuffled and transposed for easier processing
Neural Network Architecture
Input Layer: 784 nodes (representing 28Ã—28 pixel images)
Hidden Layer: 10 nodes with ReLU activation
Output Layer: 10 nodes (one per digit 0-9) with softmax activation
Key Functions
init_params(): Initializes weights and biases randomly
reLU() and deriv_relu(): Activation function and its derivative
softmax(): Converts output to probabilities
forward(): Forward propagation through the network
one_hot(): Converts digit labels to one-hot vectors
back(): Backpropagation to compute gradients
update_params(): Updates weights and biases using gradient descent
get_predictions() and get_acc(): Evaluate model performance
Training Process
The gradient_desc() function:  
Initializes random weights and biases
Performs forward propagation
Calculates error using backpropagation
Updates weights and biases
Repeats for specified number of epochs (100)
Displays accuracy every 50 epochs
This implementation demonstrates a basic neural network built using only NumPy, without using deep learning frameworks.
